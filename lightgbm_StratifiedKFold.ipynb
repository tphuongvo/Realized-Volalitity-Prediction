{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "266s1jxZFnrk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# reading file\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Modelling\n",
        "import os\n",
        "import random\n",
        "import pickle\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.cluster import KMeans\n",
        "import lightgbm as lgb\n",
        "from typing import List, Tuple, Optional, Union\n",
        "from collections import Counter, defaultdict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8qouoebff3t2"
      },
      "outputs": [],
      "source": [
        "root_path = \"data\"\n",
        "save_dir='checkpoints/artifacts/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St-nTEpA5trr"
      },
      "source": [
        "## 4.4 Lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8_ejh9ygmPvL"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(f'{root_path}/scaled_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV3PxQPfrEFP",
        "outputId": "1d7a58f7-0bf5-4825-e60b-b32efe1dbc67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 425343 entries, 0 to 425342\n",
            "Columns: 114 entries, row_id to stock_id\n",
            "dtypes: float64(112), int64(1), object(1)\n",
            "memory usage: 369.9+ MB\n"
          ]
        }
      ],
      "source": [
        "df_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "t0I4LqrXdJRL",
        "outputId": "67d15fa8-8aa2-4a25-96d1-a33db58db479"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>book.wap1.mean</th>\n",
              "      <th>book.wap1.std</th>\n",
              "      <th>book.wap1.sum</th>\n",
              "      <th>book.wap1.amin</th>\n",
              "      <th>book.wap1.amax</th>\n",
              "      <th>book.wap2.mean</th>\n",
              "      <th>book.wap2.std</th>\n",
              "      <th>book.wap2.sum</th>\n",
              "      <th>book.wap2.amin</th>\n",
              "      <th>...</th>\n",
              "      <th>trade.trade_volumn.amax</th>\n",
              "      <th>trade.weighted_price.realized_volatility</th>\n",
              "      <th>trade.weighted_price.mean</th>\n",
              "      <th>trade.weighted_price.std</th>\n",
              "      <th>trade.weighted_price.sum</th>\n",
              "      <th>trade.weighted_price.amin</th>\n",
              "      <th>trade.weighted_price.amax</th>\n",
              "      <th>target</th>\n",
              "      <th>stock_id</th>\n",
              "      <th>time_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0-5</td>\n",
              "      <td>0.533363</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.457273</td>\n",
              "      <td>0.711488</td>\n",
              "      <td>0.318289</td>\n",
              "      <td>0.006905</td>\n",
              "      <td>0.003301</td>\n",
              "      <td>0.006713</td>\n",
              "      <td>0.059265</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000685</td>\n",
              "      <td>0.221584</td>\n",
              "      <td>0.505270</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.063568</td>\n",
              "      <td>0.701747</td>\n",
              "      <td>0.334114</td>\n",
              "      <td>0.004136</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0-11</td>\n",
              "      <td>0.511839</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.288406</td>\n",
              "      <td>0.701066</td>\n",
              "      <td>0.295490</td>\n",
              "      <td>0.003434</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.002298</td>\n",
              "      <td>0.032674</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.185708</td>\n",
              "      <td>0.484476</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.047108</td>\n",
              "      <td>0.685883</td>\n",
              "      <td>0.312804</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0-16</td>\n",
              "      <td>0.507535</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.268528</td>\n",
              "      <td>0.686183</td>\n",
              "      <td>0.295734</td>\n",
              "      <td>0.001506</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.003873</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.165812</td>\n",
              "      <td>0.478549</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.038952</td>\n",
              "      <td>0.671526</td>\n",
              "      <td>0.310739</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0-31</td>\n",
              "      <td>0.503150</td>\n",
              "      <td>0.000757</td>\n",
              "      <td>0.157038</td>\n",
              "      <td>0.687526</td>\n",
              "      <td>0.293131</td>\n",
              "      <td>0.002702</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>0.094997</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.119330</td>\n",
              "      <td>0.477461</td>\n",
              "      <td>0.000729</td>\n",
              "      <td>0.022733</td>\n",
              "      <td>0.675102</td>\n",
              "      <td>0.309503</td>\n",
              "      <td>0.002195</td>\n",
              "      <td>0</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0-62</td>\n",
              "      <td>0.508009</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.248900</td>\n",
              "      <td>0.698248</td>\n",
              "      <td>0.291719</td>\n",
              "      <td>0.003609</td>\n",
              "      <td>0.001814</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.007725</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.153132</td>\n",
              "      <td>0.480999</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.034103</td>\n",
              "      <td>0.682849</td>\n",
              "      <td>0.308352</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0</td>\n",
              "      <td>62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425338</th>\n",
              "      <td>99-1962</td>\n",
              "      <td>0.507617</td>\n",
              "      <td>0.000399</td>\n",
              "      <td>0.627149</td>\n",
              "      <td>0.694414</td>\n",
              "      <td>0.293331</td>\n",
              "      <td>0.003781</td>\n",
              "      <td>0.000379</td>\n",
              "      <td>0.005243</td>\n",
              "      <td>0.167695</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005762</td>\n",
              "      <td>0.316797</td>\n",
              "      <td>0.480570</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.120067</td>\n",
              "      <td>0.678839</td>\n",
              "      <td>0.310094</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>99</td>\n",
              "      <td>1962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425339</th>\n",
              "      <td>99-1980</td>\n",
              "      <td>0.496448</td>\n",
              "      <td>0.005809</td>\n",
              "      <td>0.871126</td>\n",
              "      <td>0.631582</td>\n",
              "      <td>0.347846</td>\n",
              "      <td>0.003782</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.007226</td>\n",
              "      <td>0.006395</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026894</td>\n",
              "      <td>0.726181</td>\n",
              "      <td>0.468218</td>\n",
              "      <td>0.005909</td>\n",
              "      <td>0.560080</td>\n",
              "      <td>0.614216</td>\n",
              "      <td>0.364131</td>\n",
              "      <td>0.005230</td>\n",
              "      <td>99</td>\n",
              "      <td>1980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425340</th>\n",
              "      <td>99-1981</td>\n",
              "      <td>0.529883</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.757741</td>\n",
              "      <td>0.711353</td>\n",
              "      <td>0.328254</td>\n",
              "      <td>0.002148</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.003874</td>\n",
              "      <td>0.031994</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003076</td>\n",
              "      <td>0.560824</td>\n",
              "      <td>0.502123</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.341909</td>\n",
              "      <td>0.696562</td>\n",
              "      <td>0.343720</td>\n",
              "      <td>0.004505</td>\n",
              "      <td>99</td>\n",
              "      <td>1981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425341</th>\n",
              "      <td>99-1991</td>\n",
              "      <td>0.506296</td>\n",
              "      <td>0.000249</td>\n",
              "      <td>0.741608</td>\n",
              "      <td>0.694887</td>\n",
              "      <td>0.291730</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.004097</td>\n",
              "      <td>0.190385</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003259</td>\n",
              "      <td>0.289489</td>\n",
              "      <td>0.479031</td>\n",
              "      <td>0.000277</td>\n",
              "      <td>0.102198</td>\n",
              "      <td>0.679863</td>\n",
              "      <td>0.308145</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>99</td>\n",
              "      <td>1991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425342</th>\n",
              "      <td>99-1998</td>\n",
              "      <td>0.528424</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.810131</td>\n",
              "      <td>0.708782</td>\n",
              "      <td>0.321692</td>\n",
              "      <td>0.003488</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>0.006236</td>\n",
              "      <td>0.003751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005894</td>\n",
              "      <td>0.612522</td>\n",
              "      <td>0.500404</td>\n",
              "      <td>0.000987</td>\n",
              "      <td>0.403653</td>\n",
              "      <td>0.694017</td>\n",
              "      <td>0.339814</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>99</td>\n",
              "      <td>1998</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>425343 rows × 115 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         row_id  book.wap1.mean  book.wap1.std  book.wap1.sum  book.wap1.amin  \\\n",
              "0           0-5        0.533363       0.000693       0.457273        0.711488   \n",
              "1          0-11        0.511839       0.000262       0.288406        0.701066   \n",
              "2          0-16        0.507535       0.000864       0.268528        0.686183   \n",
              "3          0-31        0.503150       0.000757       0.157038        0.687526   \n",
              "4          0-62        0.508009       0.000258       0.248900        0.698248   \n",
              "...         ...             ...            ...            ...             ...   \n",
              "425338  99-1962        0.507617       0.000399       0.627149        0.694414   \n",
              "425339  99-1980        0.496448       0.005809       0.871126        0.631582   \n",
              "425340  99-1981        0.529883       0.001138       0.757741        0.711353   \n",
              "425341  99-1991        0.506296       0.000249       0.741608        0.694887   \n",
              "425342  99-1998        0.528424       0.000999       0.810131        0.708782   \n",
              "\n",
              "        book.wap1.amax  book.wap2.mean  book.wap2.std  book.wap2.sum  \\\n",
              "0             0.318289        0.006905       0.003301       0.006713   \n",
              "1             0.295490        0.003434       0.000518       0.002298   \n",
              "2             0.295734        0.001506       0.000182       0.001055   \n",
              "3             0.293131        0.002702       0.000438       0.001059   \n",
              "4             0.291719        0.003609       0.001814       0.002100   \n",
              "...                ...             ...            ...            ...   \n",
              "425338        0.293331        0.003781       0.000379       0.005243   \n",
              "425339        0.347846        0.003782       0.001613       0.007226   \n",
              "425340        0.328254        0.002148       0.000342       0.003874   \n",
              "425341        0.291730        0.002348       0.000109       0.004097   \n",
              "425342        0.321692        0.003488       0.001052       0.006236   \n",
              "\n",
              "        book.wap2.amin  ...  trade.trade_volumn.amax  \\\n",
              "0             0.059265  ...                 0.000685   \n",
              "1             0.032674  ...                 0.000383   \n",
              "2             0.003873  ...                 0.000534   \n",
              "3             0.094997  ...                 0.000614   \n",
              "4             0.007725  ...                 0.000466   \n",
              "...                ...  ...                      ...   \n",
              "425338        0.167695  ...                 0.005762   \n",
              "425339        0.006395  ...                 0.026894   \n",
              "425340        0.031994  ...                 0.003076   \n",
              "425341        0.190385  ...                 0.003259   \n",
              "425342        0.003751  ...                 0.005894   \n",
              "\n",
              "        trade.weighted_price.realized_volatility  trade.weighted_price.mean  \\\n",
              "0                                       0.221584                   0.505270   \n",
              "1                                       0.185708                   0.484476   \n",
              "2                                       0.165812                   0.478549   \n",
              "3                                       0.119330                   0.477461   \n",
              "4                                       0.153132                   0.480999   \n",
              "...                                          ...                        ...   \n",
              "425338                                  0.316797                   0.480570   \n",
              "425339                                  0.726181                   0.468218   \n",
              "425340                                  0.560824                   0.502123   \n",
              "425341                                  0.289489                   0.479031   \n",
              "425342                                  0.612522                   0.500404   \n",
              "\n",
              "        trade.weighted_price.std  trade.weighted_price.sum  \\\n",
              "0                       0.000578                  0.063568   \n",
              "1                       0.000304                  0.047108   \n",
              "2                       0.000932                  0.038952   \n",
              "3                       0.000729                  0.022733   \n",
              "4                       0.000182                  0.034103   \n",
              "...                          ...                       ...   \n",
              "425338                  0.000435                  0.120067   \n",
              "425339                  0.005909                  0.560080   \n",
              "425340                  0.001087                  0.341909   \n",
              "425341                  0.000277                  0.102198   \n",
              "425342                  0.000987                  0.403653   \n",
              "\n",
              "        trade.weighted_price.amin  trade.weighted_price.amax    target  \\\n",
              "0                        0.701747                   0.334114  0.004136   \n",
              "1                        0.685883                   0.312804  0.001445   \n",
              "2                        0.671526                   0.310739  0.002168   \n",
              "3                        0.675102                   0.309503  0.002195   \n",
              "4                        0.682849                   0.308352  0.001747   \n",
              "...                           ...                        ...       ...   \n",
              "425338                   0.678839                   0.310094  0.001122   \n",
              "425339                   0.614216                   0.364131  0.005230   \n",
              "425340                   0.696562                   0.343720  0.004505   \n",
              "425341                   0.679863                   0.308145  0.001242   \n",
              "425342                   0.694017                   0.339814  0.003059   \n",
              "\n",
              "        stock_id  time_id  \n",
              "0              0        5  \n",
              "1              0       11  \n",
              "2              0       16  \n",
              "3              0       31  \n",
              "4              0       62  \n",
              "...          ...      ...  \n",
              "425338        99     1962  \n",
              "425339        99     1980  \n",
              "425340        99     1981  \n",
              "425341        99     1991  \n",
              "425342        99     1998  \n",
              "\n",
              "[425343 rows x 115 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train['time_id'] = df_train['row_id'].str.split('-', expand = True)[1]\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_X_y(data):\n",
        " Xfeature = data.drop(columns = [\"target\",\"time_id\",\"row_id\"])\n",
        " y = data[\"target\"]\n",
        " return Xfeature, y\n",
        " \n",
        "\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "\n",
        "def feval_rmspe(y_pred, model, is_xgb=True):\n",
        "    y_true = model.get_label()\n",
        "\n",
        "    if is_xgb:\n",
        "        return \"RMSPE\", rmspe(y_true, y_pred)\n",
        "\n",
        "    return \"RMSPE\", rmspe(y_true, y_pred), False\n",
        "\n",
        "\n",
        "def feval_wrapper(y_pred, model):\n",
        "    return feval_rmspe(y_pred, model, is_xgb=False)\n",
        "\n",
        "\n",
        "def stratified_group_k_fold(X, y, groups, k, seed=None):\n",
        "    \"\"\" https://www.kaggle.com/jakubwasikowski/stratified-group-k-fold-cross-validation \"\"\"\n",
        "    labels_num = np.max(y) + 1\n",
        "    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n",
        "    y_distr = Counter()\n",
        "    for label, g in zip(y, groups):\n",
        "        y_counts_per_group[g][label] += 1\n",
        "        y_distr[label] += 1\n",
        "\n",
        "    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n",
        "    groups_per_fold = defaultdict(set)\n",
        "\n",
        "    def eval_y_counts_per_fold(y_counts, fold):\n",
        "        y_counts_per_fold[fold] += y_counts\n",
        "        std_per_label = []\n",
        "        for label in range(labels_num):\n",
        "            label_std = np.std([y_counts_per_fold[i][label] / y_distr[label] for i in range(k)])\n",
        "            std_per_label.append(label_std)\n",
        "        y_counts_per_fold[fold] -= y_counts\n",
        "        return np.mean(std_per_label)\n",
        "    \n",
        "    groups_and_y_counts = list(y_counts_per_group.items())\n",
        "    random.Random(seed).shuffle(groups_and_y_counts)\n",
        "\n",
        "    for g, y_counts in tqdm(sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])), total=len(groups_and_y_counts)):\n",
        "        best_fold = None\n",
        "        min_eval = None\n",
        "        for i in range(k):\n",
        "            fold_eval = eval_y_counts_per_fold(y_counts, i)\n",
        "            if min_eval is None or fold_eval < min_eval:\n",
        "                min_eval = fold_eval\n",
        "                best_fold = i\n",
        "        y_counts_per_fold[best_fold] += y_counts\n",
        "        groups_per_fold[best_fold].add(g)\n",
        "\n",
        "    all_groups = set(groups)\n",
        "    for i in range(k):\n",
        "        train_groups = all_groups - groups_per_fold[i]\n",
        "        test_groups = groups_per_fold[i]\n",
        "\n",
        "        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n",
        "        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n",
        "\n",
        "        yield train_indices, test_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "lgb_bl = {\n",
        "    \"boosting_type\": [\"gbdt\"],\n",
        "    \"max_depth\": [7],\n",
        "    \"num_leaves\": [128],\n",
        "    #\"early_stopping_rounds\":[10],\n",
        "\n",
        "    \"learning_rate\": [0.05],\n",
        "    \"subsample\": [0.72],\n",
        "    \"subsample_freq\": [4],\n",
        "    \"feature_fraction\": [0.4],\n",
        "    \"feature_fraction_bynode\": [0.8],\n",
        "    \"bagging_fraction\": [0.75],\n",
        "    \"bagging_freq\": [25],\n",
        "\n",
        "    \"min_data_in_leaf\": [1000],\n",
        "    \"min_sum_hessian_in_leaf\": [20],\n",
        "\n",
        "    \"lambda_l1\": [2],\n",
        "    \"lambda_l2\": [4],\n",
        "        \n",
        "    \"extra_trees\": [True],\n",
        "    \"force_col_wise\": [True],\n",
        "        \n",
        "    \"categorical_column\": [0],\n",
        "    \"n_jobs\": [-1],\n",
        "    \"verbose\": [-1],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_lgbm(df_train: pd.DataFrame, l ,output_dir,params):\n",
        "    global feval_wrapper\n",
        "\n",
        "    best_losses = []\n",
        "    best_predictions = []\n",
        "    best_model = None\n",
        "\n",
        "    for i in l:\n",
        "      print(f'stock group: {i}')\n",
        "      X_stock = df_train[df_train.stock_id.isin(i)]\n",
        "\n",
        "      # Create out of folds array\n",
        "      y = X_stock['target']\n",
        "      X_stock = X_stock.drop(['target'], axis=1)\n",
        "      # Iterate through each fold\n",
        "\n",
        "\n",
        "      if CV_SPLIT == 'default':\n",
        "        #gkf = GroupKFold(n_splits=KFOLD)\n",
        "        skf = stratified_group_k_fold(X=X_stock[feats_nostock], y=X_stock['stock_id'].astype('category').cat.codes.values, \n",
        "                                  groups=np.array(X_stock['time_id'].astype('category').cat.codes.values), k=KFOLD, seed=try_seed)\n",
        "        folds = []\n",
        "\n",
        "        for cv_idx, (idx_train, idx_valid) in enumerate(skf):\n",
        "    # for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n",
        "          X_tr, X_va = X_stock.iloc[idx_train], X_stock.iloc[idx_valid]\n",
        "          y_tr, y_va = y.iloc[idx_train], y.iloc[idx_valid]\n",
        "\n",
        "          \n",
        "          X_tr = X_tr.drop(columns = [\"time_id\", \"row_id\"], axis=1)\n",
        "          X_va = X_va.drop(columns = [\"time_id\", \"row_id\"], axis=1)\n",
        "          dtrain = lgb.Dataset(X_tr, y_tr, weight=1/np.square(y_tr))\n",
        "          dval = lgb.Dataset(X_va, y_va, weight=1/np.square(y_va))\n",
        "\n",
        "          best_loss = 1e10\n",
        "          best_prediction = None\n",
        "\n",
        "          print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n",
        "\n",
        "          model = lgb.train(params=params,\n",
        "                                num_boost_round=10000,\n",
        "                                train_set=dtrain,\n",
        "                                valid_sets=[dtrain, dval],\n",
        "                                verbose_eval=250,\n",
        "                                early_stopping_rounds=200,\n",
        "                                #early_stopping_rounds=1,\n",
        "                                feval=feval_wrapper)\n",
        "\n",
        "          fold_preds = model.predict(X_va)\n",
        "          valid_rmspe = rmspe(y_va, fold_preds)\n",
        "          #print(f\"\\nvalid rmspe of fold {fold}: {valid_rmspe}\")\n",
        "          \n",
        "          print(f\"\\nRMSPE of fold {cv_idx}: {valid_rmspe}\")\n",
        "          # import pdb; pdb.set_trace()\n",
        "          pickle.dump(model, open(os.path.join(output_dir, f\"lgb_bl_{cv_idx}.pkl\"), \"wb\"))\n",
        "          \n",
        "          \n",
        "          # import pdb; pdb.set_trace()\n",
        "          best_predictions.append(fold_preds)\n",
        "          best_losses.append(valid_rmspe)\n",
        "\n",
        "    return best_losses, best_predictions          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VwMZeJrIulNt"
      },
      "outputs": [],
      "source": [
        "# # # full data\n",
        "X, y = get_X_y(df_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VlBYnijrYUYz"
      },
      "outputs": [],
      "source": [
        "features = [col for col in df_train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
        "feats_nostock = [col for col in df_train.columns if col not in {\"time_id\", \"target\", \"row_id\", \"stock_id\"}] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4z3vtUsYhuv",
        "outputId": "1a84c527-b2b4-4c6c-baec-6b16d9152047"
      },
      "outputs": [],
      "source": [
        "# # Create out of folds array\n",
        "# y = df_train['target']\n",
        "# # Iterate through each fold\n",
        "# try_seed = 42\n",
        "# KFOLD = 5\n",
        "# CV_SPLIT = 'default' \n",
        "\n",
        "# if CV_SPLIT == 'default':\n",
        "#     #gkf = GroupKFold(n_splits=KFOLD)\n",
        "#     skf = stratified_group_k_fold(X=df_train[feats_nostock], y=df_train['stock_id'].astype('category').cat.codes.values, \n",
        "#                               groups=np.array(df_train['time_id'].astype('category').cat.codes.values), k=KFOLD, seed=try_seed)\n",
        "#     folds = []\n",
        "\n",
        "#     for i, (idx_train, idx_valid) in enumerate(skf):\n",
        "#       # x_train, x_val = train.iloc[idx_train], train.iloc[idx_valid]\n",
        "#       # y_train, y_val = y.iloc[idx_train], y.iloc[idx_valid]\n",
        "#       folds.append((idx_train, idx_valid))\n",
        "#       print(f'Fold {i}: {len(idx_train)} train, {len(idx_valid)} valid')\n",
        "# else:\n",
        "#     raise ValueError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWDbLDnJXtQh",
        "outputId": "f4e4689f-793a-4178-8d64-97bc97dc0ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 1 2 4 0 0 4 2 3 4 1 1 2 2 2 2 2 4 2 2 0 1 2 2 0 2 1 2 4 2 0 2 2 1 0 0 2\n",
            " 0 2 2 2 1 2 2 2 1 2 2 2 1 1 4 2 0 4 1 1 2 2 2 2 1 2 1 1 0 4 1 4 1 3 3 0 0\n",
            " 1 2 0 1 0 4 0 2 2 2 1 4 0 0 2 0 2 0 2 2 2 2 2 4 2 1 1 2 2 1 2 2 2 1 2 1 2\n",
            " 1]\n"
          ]
        }
      ],
      "source": [
        "train_p = pd.read_csv(f'{root_path}/train.csv')\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr(method='kendall')\n",
        "\n",
        "ids = corr.index\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, random_state=42).fit(corr.values)\n",
        "print(kmeans.labels_) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw84kVJAZ7P6",
        "outputId": "ed3ae5b2-b3af-41ad-9c3a-7db12d6b41a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stock group: [4, 5, 21, 27, 33, 37, 38, 40, 60, 74, 82, 83, 86, 88, 90, 98, 99, 101, 103]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3830/3830 [00:27<00:00, 138.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0: 55333 train, 13833 valid\n",
            "Fold 1: 55333 train, 13833 valid\n",
            "Fold 2: 55333 train, 13833 valid\n",
            "Fold 3: 55333 train, 13833 valid\n",
            "Fold 4: 55332 train, 13834 valid\n",
            "stock group: [1, 10, 11, 22, 29, 36, 44, 50, 55, 56, 62, 63, 69, 72, 73, 76, 78, 84, 87, 96, 112, 113, 116, 122, 124, 126]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3830/3830 [00:48<00:00, 79.00it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0: 79664 train, 19916 valid\n",
            "Fold 1: 79664 train, 19916 valid\n",
            "Fold 2: 79664 train, 19916 valid\n",
            "Fold 3: 79664 train, 19916 valid\n",
            "Fold 4: 79664 train, 19916 valid\n",
            "stock group: [0, 2, 7, 13, 14, 15, 16, 17, 19, 20, 23, 26, 28, 30, 32, 34, 35, 39, 41, 42, 43, 46, 47, 48, 51, 52, 53, 59, 64, 66, 67, 68, 70, 85, 93, 94, 95, 100, 102, 104, 105, 107, 108, 109, 111, 114, 115, 118, 119, 120, 123, 125]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3830/3830 [01:53<00:00, 33.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0: 159327 train, 39831 valid\n",
            "Fold 1: 159327 train, 39831 valid\n",
            "Fold 2: 159326 train, 39832 valid\n",
            "Fold 3: 159326 train, 39832 valid\n",
            "Fold 4: 159326 train, 39832 valid\n",
            "stock group: [8, 80, 81]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3830/3830 [00:08<00:00, 464.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0: 9184 train, 2296 valid\n",
            "Fold 1: 9184 train, 2296 valid\n",
            "Fold 2: 9184 train, 2296 valid\n",
            "Fold 3: 9184 train, 2296 valid\n",
            "Fold 4: 9184 train, 2296 valid\n",
            "stock group: [3, 6, 9, 18, 31, 58, 61, 75, 77, 89, 97, 110]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3830/3830 [00:22<00:00, 167.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0: 36768 train, 9191 valid\n",
            "Fold 1: 36767 train, 9192 valid\n",
            "Fold 2: 36767 train, 9192 valid\n",
            "Fold 3: 36767 train, 9192 valid\n",
            "Fold 4: 36767 train, 9192 valid\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "try_seed = 42\n",
        "KFOLD = 5\n",
        "CV_SPLIT = 'default' \n",
        "\n",
        "l = []\n",
        "for n in range(5):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "\n",
        "for i in l:\n",
        "  print(f'stock group: {i}')\n",
        "  X_stock = df_train[df_train.stock_id.isin(i)]\n",
        "\n",
        "  # Create out of folds array\n",
        "  y = X_stock['target']\n",
        "  X_stock = X_stock.drop(['target'], axis=1)\n",
        "  # Iterate through each fold\n",
        "\n",
        "\n",
        "  if CV_SPLIT == 'default':\n",
        "    #gkf = GroupKFold(n_splits=KFOLD)\n",
        "    skf = stratified_group_k_fold(X=X_stock[feats_nostock], y=X_stock['stock_id'].astype('category').cat.codes.values, \n",
        "                              groups=np.array(X_stock['time_id'].astype('category').cat.codes.values), k=KFOLD, seed=try_seed)\n",
        "    folds = []\n",
        "\n",
        "    for i, (idx_train, idx_valid) in enumerate(skf):\n",
        "      # x_train, x_val = train.iloc[idx_train], train.iloc[idx_valid]\n",
        "      # y_train, y_val = y.iloc[idx_train], y.iloc[idx_valid]\n",
        "      folds.append((idx_train, idx_valid))\n",
        "      print(f'Fold {i}: {len(idx_train)} train, {len(idx_valid)} valid')\n",
        "  else:\n",
        "      raise ValueError()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PKB8z4__3v6x"
      },
      "outputs": [],
      "source": [
        "X.reset_index(drop=True, inplace=True); y.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOt6V7Prp6ey",
        "outputId": "fa7abb98-e4a0-44e9-d91d-c4dd755cb973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stock group: [4, 5, 21, 27, 33, 37, 38, 40, 60, 74, 82, 83, 86, 88, 90, 98, 99, 101, 103]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3830/3830 [00:54<00:00, 69.66it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fold 0 train: (55333, 112), valid: (13833, 112)\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[250]\ttraining's RMSPE: 0.263012\tvalid_1's RMSPE: 0.264363\n",
            "[500]\ttraining's RMSPE: 0.257579\tvalid_1's RMSPE: 0.26244\n",
            "[750]\ttraining's RMSPE: 0.253796\tvalid_1's RMSPE: 0.261456\n",
            "[1000]\ttraining's RMSPE: 0.250743\tvalid_1's RMSPE: 0.260964\n",
            "[1250]\ttraining's RMSPE: 0.248086\tvalid_1's RMSPE: 0.2608\n",
            "[1500]\ttraining's RMSPE: 0.245577\tvalid_1's RMSPE: 0.260567\n",
            "[1750]\ttraining's RMSPE: 0.243473\tvalid_1's RMSPE: 0.26047\n",
            "[2000]\ttraining's RMSPE: 0.24151\tvalid_1's RMSPE: 0.260505\n",
            "Early stopping, best iteration is:\n",
            "[1837]\ttraining's RMSPE: 0.242767\tvalid_1's RMSPE: 0.260375\n",
            "\n",
            "RMSPE of fold 0: 0.2603750203088708\n",
            "fold 1 train: (55333, 112), valid: (13833, 112)\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.72 will be ignored. Current value: bagging_fraction=0.75\n",
            "[LightGBM] [Warning] bagging_freq is set=25, subsample_freq=4 will be ignored. Current value: bagging_freq=25\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[250]\ttraining's RMSPE: 0.261754\tvalid_1's RMSPE: 0.273977\n",
            "[500]\ttraining's RMSPE: 0.256318\tvalid_1's RMSPE: 0.271675\n",
            "[750]\ttraining's RMSPE: 0.25239\tvalid_1's RMSPE: 0.270473\n",
            "[1000]\ttraining's RMSPE: 0.249292\tvalid_1's RMSPE: 0.269954\n",
            "[1250]\ttraining's RMSPE: 0.246617\tvalid_1's RMSPE: 0.269663\n",
            "[1500]\ttraining's RMSPE: 0.244285\tvalid_1's RMSPE: 0.269517\n",
            "[1750]\ttraining's RMSPE: 0.242158\tvalid_1's RMSPE: 0.269529\n",
            "Early stopping, best iteration is:\n",
            "[1556]\ttraining's RMSPE: 0.243785\tvalid_1's RMSPE: 0.269399\n",
            "\n",
            "RMSPE of fold 1: 0.26939949881674846\n",
            "fold 2 train: (55333, 112), valid: (13833, 112)\n",
            "[LightGBM] [Warning] bagging_fraction is set=0.75, subsample=0.72 will be ignored. Current value: bagging_fraction=0.75\n",
            "[LightGBM] [Warning] bagging_freq is set=25, subsample_freq=4 will be ignored. Current value: bagging_freq=25\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[250]\ttraining's RMSPE: 0.262444\tvalid_1's RMSPE: 0.27009\n",
            "[500]\ttraining's RMSPE: 0.256836\tvalid_1's RMSPE: 0.26773\n",
            "[750]\ttraining's RMSPE: 0.25308\tvalid_1's RMSPE: 0.266524\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_3776/3654757715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m lgbm_losses, lgbm_preds = train_lgbm(df_train,l = l, \n\u001b[1;32m      2\u001b[0m                               \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                               params= lgb_bl)\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_3776/1462917644.py\u001b[0m in \u001b[0;36mtrain_lgbm\u001b[0;34m(df_train, l, output_dir, params)\u001b[0m\n\u001b[1;32m     45\u001b[0m                                 \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                 \u001b[0;31m#early_stopping_rounds=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                                 feval=feval_wrapper)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mfold_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_va\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/learning/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalid_sets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                 \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mevaluation_result_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/learning/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36meval_train\u001b[0;34m(self, feval)\u001b[0m\n\u001b[1;32m   2854\u001b[0m             \u001b[0mList\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2855\u001b[0m         \"\"\"\n\u001b[0;32m-> 2856\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_data_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/miniconda3/envs/learning/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36m__inner_eval\u001b[0;34m(self, data_name, data_idx, feval)\u001b[0m\n\u001b[1;32m   3400\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3401\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3402\u001b[0;31m                 \u001b[0mfeval_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__inner_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3403\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeval_ret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0meval_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_higher_better\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeval_ret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3776/105708136.py\u001b[0m in \u001b[0;36mfeval_wrapper\u001b[0;34m(y_pred, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfeval_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeval_rmspe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_xgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3776/105708136.py\u001b[0m in \u001b[0;36mfeval_rmspe\u001b[0;34m(y_pred, model, is_xgb)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"RMSPE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmspe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"RMSPE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmspe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_3776/105708136.py\u001b[0m in \u001b[0;36mrmspe\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrmspe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lgbm_losses, lgbm_preds = train_lgbm(df_train,l = l, \n",
        "                              output_dir=save_dir,\n",
        "                              params= lgb_bl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRsApnevm4Bd",
        "outputId": "4cd00bbc-d7c1-4e2e-b4a3-6f7bc8c04e41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.2281209816687572,\n",
              " 0.21700990661736327,\n",
              " 0.22441095293406976,\n",
              " 0.2170015148121175,\n",
              " 0.22123825265447938,\n",
              " 0.2690769809335837,\n",
              " 0.2642502810387935,\n",
              " 0.26503750437013474,\n",
              " 0.28367435149565395,\n",
              " 0.2643990561289135,\n",
              " 0.2220524477783735,\n",
              " 0.2207926229267815,\n",
              " 0.22233575068123035,\n",
              " 0.21904276479078513,\n",
              " 0.22546162648628454,\n",
              " 0.24420483359555645,\n",
              " 0.24961161451136596,\n",
              " 0.28861180306826734,\n",
              " 0.2493914839034596,\n",
              " 0.25357001073907326,\n",
              " 0.20484109119198965,\n",
              " 0.19257327598365936,\n",
              " 0.197961206000214,\n",
              " 0.19132198979071394,\n",
              " 0.19350023119613577]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lgbm_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUKqhFM0gpLb",
        "outputId": "3c105b4a-1f66-43c6-c44c-cfe1b8901b3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.23317970141191025"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(lgbm_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Uv0Yh23N9d"
      },
      "outputs": [],
      "source": [
        "# 0.2324505127007035"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3NPecV5ab2r"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iPK2jvqRF7v"
      },
      "outputs": [],
      "source": [
        "# with open(os.path.join(save_dir, f\"lgb_bl_0.pkl\"), 'rb') as f:\n",
        "#    model_0 = pickle.load(f) \n",
        "# with open(os.path.join(save_dir, f\"lgb_bl_1.pkl\"), 'rb') as f:\n",
        "#    model_1 = pickle.load(f) \n",
        "# with open(os.path.join(save_dir, f\"lgb_bl_2.pkl\"), 'rb') as f:\n",
        "#    model_2 = pickle.load(f) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lightgbm  StratifiedKFold.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "93a52d2716d37cb936144ccca0f460bb19246a32b6727480f5deda6579d2e26e"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 ('learning')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
